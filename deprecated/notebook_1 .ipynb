{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2K66vKOcGwE"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MazsGprpcGwH"
      },
      "outputs": [],
      "source": [
        "from  utils import read_txt_file, write_csv_file\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eldX2yFcGwJ"
      },
      "source": [
        "# Materials and Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USECRE_KcGwJ"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqBDss3AcGwK"
      },
      "source": [
        "The dataset **RB198** was employed as the training set, while **RB111** served as the independent set in this implementation.\n",
        "\n",
        " Both datasets were acquired from the following source: http://ailab-projects2.ist.psu.edu/RNABindRPlus/data.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X13nkSp4cGwK"
      },
      "source": [
        "Originally, the data was structured as a text file in the following format:\n",
        "\n",
        "```\n",
        "#First line: PDBID and Chain ID\n",
        "#Second line: Sequence\n",
        "#Third line: Interface residues defined using a 5.0 angstrom distance cut-off\n",
        "2XFZ_Y\n",
        "KGFKDYGHDYHPAPKTENIKGLGDLKPGIPKTPKQNGGGKRKRWTGDKGRKIYEWDSQAGELEGYRASDGQHLGSFDPKTGNQLKGPDPKRNIKKYL\n",
        "0000000011100000000000000000111011111111110000010110000111100000010110000000000000000000111101111\n",
        "```\n",
        "\n",
        "We needed to convert this data into a CSV file with the following features:\n",
        "\n",
        "**PDBID**, **ChainID**, **Sequence**, **Interface**\n",
        "\n",
        "The script processed this transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN050rE9cGwL"
      },
      "outputs": [],
      "source": [
        "rb198txt = 'Datasets/RB198.txt'\n",
        "rb198 = 'Datasets/RB198.csv'\n",
        "\n",
        "data = read_txt_file(rb198txt)\n",
        "write_csv_file(data, rb198)\n",
        "\n",
        "rb111txt = 'Datasets/RB111.txt'\n",
        "rb111 = 'Datasets/RB111.csv'\n",
        "data = read_txt_file(rb111txt)\n",
        "write_csv_file(data, rb111)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN1KUEjIcGwL"
      },
      "source": [
        "The structure of the dataframe resembles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EH9UQCa8cGwM",
        "outputId": "83a2d001-514e-4ff2-c834-dd7f7b39c231"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PDBID</th>\n",
              "      <th>ChainID</th>\n",
              "      <th>Sequence</th>\n",
              "      <th>Interface</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2AZ0</td>\n",
              "      <td>A</td>\n",
              "      <td>MPSKLALIQELPDRIQTAVEAAMGMSYQDAPNNVRRDLDNLHACLN...</td>\n",
              "      <td>0000000000000000000000000000000010011001001100...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1M8V</td>\n",
              "      <td>A</td>\n",
              "      <td>GAMAERPLDVIHRSLDKDVLVILKKGFEFRGRLIGYDIHLNVVLAD...</td>\n",
              "      <td>0001110110010010000000000000000001111110000000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2PJP</td>\n",
              "      <td>A</td>\n",
              "      <td>FSEEQQAIWQKAEPLFGDEPWWVRDLAKETGTDEQAMRLTLRQAAQ...</td>\n",
              "      <td>0000000000000000000001110000000001000100000000...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  PDBID ChainID                                           Sequence  \\\n",
              "0  2AZ0       A  MPSKLALIQELPDRIQTAVEAAMGMSYQDAPNNVRRDLDNLHACLN...   \n",
              "1  1M8V       A  GAMAERPLDVIHRSLDKDVLVILKKGFEFRGRLIGYDIHLNVVLAD...   \n",
              "2  2PJP       A  FSEEQQAIWQKAEPLFGDEPWWVRDLAKETGTDEQAMRLTLRQAAQ...   \n",
              "\n",
              "                                           Interface  \n",
              "0  0000000000000000000000000000000010011001001100...  \n",
              "1  0001110110010010000000000000000001111110000000...  \n",
              "2  0000000000000000000001110000000001000100000000...  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = pd.read_csv('Datasets/RB198.csv')\n",
        "train_data.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjr-sFSicGwO"
      },
      "source": [
        "## Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGu-F_BtcGwO"
      },
      "source": [
        "In this implementation, the proposed PRIP method comprised five steps:\n",
        "\n",
        "1. Pre-training the Word2vec model.\n",
        "2. Dividing protein sequences.\n",
        "3. Extracting semantic features.\n",
        "4. Training the XGBoost classifier.\n",
        "5. Discerning between binding and non-binding sites.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lolfWfSQcGwP"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km48R1BVcGwQ"
      },
      "outputs": [],
      "source": [
        "def split_protein_sequence(protein_sequence, segment_length):\n",
        "\n",
        "    if segment_length % 2 == 0 :\n",
        "        raise ValueError(\"Segment length must be an odd number and greater than or equal to 2*n + 1\")\n",
        "\n",
        "    segments = []\n",
        "    sequence_length = len(protein_sequence)\n",
        "\n",
        "    n = (segment_length - 1)// 2\n",
        "\n",
        "    # Iterate over each residue in the protein sequence\n",
        "    for i in range(sequence_length):\n",
        "        # Determine the start and end indices of the segment\n",
        "        start_index = max(0, i - n)\n",
        "        end_index = min(sequence_length, i + n + 1)\n",
        "\n",
        "        # Pad the segment with \"X\" characters at the start or end if needed\n",
        "        if start_index == 0:\n",
        "            padded_segment = \"X\" * (n - i) + protein_sequence[:end_index]\n",
        "        elif end_index == sequence_length:\n",
        "            padded_segment = protein_sequence[start_index:] + \"X\" * (n - (sequence_length - 1 - i))\n",
        "        else:\n",
        "            segment = protein_sequence[start_index:end_index]\n",
        "            padded_segment = \"X\" * (n - (i - start_index)) + segment\n",
        "\n",
        "        # Add the segment to the list of segments\n",
        "        segments.append(padded_segment)\n",
        "\n",
        "    return segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt5xtsdjcGwQ"
      },
      "outputs": [],
      "source": [
        "# Concatenate semantic vectors for each residue\n",
        "def feature_extract(segments , W2V_model):\n",
        "\n",
        "    semantic_vect_concat = []\n",
        "    for segment in segments :\n",
        "        seg_vect = []\n",
        "        for residue in segment :\n",
        "            if residue != 'X' :\n",
        "                seg_vect.extend(W2V_model.wv[residue])\n",
        "            else:\n",
        "                seg_vect.extend([0 for i in range(25)])\n",
        "        semantic_vect_concat.append(seg_vect)\n",
        "\n",
        "    return semantic_vect_concat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdn6MQXAcGwR"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "\n",
        "def load_model(path):\n",
        "    return Word2Vec.load(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlfVq-lpcGwR"
      },
      "outputs": [],
      "source": [
        "model = load_model(\"model_W2V.model\")\n",
        "\n",
        "df_train = pd.read_csv(\"RB198.csv\")\n",
        "df_test = pd.read_csv(\"RB111.csv\")\n",
        "\n",
        "protein_seqs_train = df_train['Sequence'].to_list()\n",
        "label_interfaces_train = df_train['Interface'].to_list()\n",
        "\n",
        "\n",
        "protein_seqs_test= df_test['Sequence'].to_list()\n",
        "label_interfaces_test = df_test['Interface'].to_list()\n",
        "\n",
        "pairs_train = list(zip(protein_seqs_train , label_interfaces_train))\n",
        "pairs_test = list(zip(protein_seqs_test , label_interfaces_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzptIZUFcGwS"
      },
      "outputs": [],
      "source": [
        "# Sequence splitting + assigning the interface labels for each split\n",
        "X_train , y_train  , X_test , y_test = [] , [] , [] , []\n",
        "segment_length = 11\n",
        "n = 3\n",
        "for elem in pairs_train :\n",
        "    prot_sequence = elem[0]\n",
        "    label_interf = elem[1]\n",
        "\n",
        "    if len(prot_sequence) < segment_length :\n",
        "        print(\"inf\")\n",
        "    segments = split_protein_sequence(prot_sequence ,segment_length)\n",
        "\n",
        "    # X represents the segments (sequences split) and y represents the respective labels\n",
        "    X_train.extend(segments)\n",
        "    y_train.extend(label_interf)\n",
        "\n",
        "for elem in pairs_test :\n",
        "    prot_sequence = elem[0]\n",
        "    label_interf = elem[1]\n",
        "\n",
        "    if len(prot_sequence) < segment_length :\n",
        "        print(\"inf\")\n",
        "    segments = split_protein_sequence(prot_sequence ,segment_length)\n",
        "\n",
        "    # X represents the segments (sequences split) and y represents the respective labels\n",
        "    X_test.extend(segments)\n",
        "    y_test.extend(label_interf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sir4RDQWcGwS"
      },
      "outputs": [],
      "source": [
        "semantic_vects_train = feature_extract(X_train , model)\n",
        "semantic_vects_test = feature_extract(X_test , model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyvLvL7gcGwT"
      },
      "outputs": [],
      "source": [
        "y_train_int = [int(x) for x in y_train]\n",
        "y_test_int = [int(x) for x in y_test]\n",
        "\n",
        "y_train = y_train_int\n",
        "y_test = y_test_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skNm8NMQcGwT"
      },
      "outputs": [],
      "source": [
        "def sensitivity(y_true, y_pred):\n",
        "    eps = 1e-8\n",
        "    TP = ((y_true == 1) & (y_pred == 1)).sum()\n",
        "    FN = ((y_true == 1) & (y_pred == 0)).sum()\n",
        "    return TP / (TP + FN + eps)\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    eps = 1e-8\n",
        "    TN = ((y_true == 0) & (y_pred == 0)).sum()\n",
        "    FP = ((y_true == 0) & (y_pred == 1)).sum()\n",
        "    return TN / (TN + FP + eps)\n",
        "def matthews_coeff(y_true , y_pred):\n",
        "    eps = 1e-8\n",
        "    TN = ((y_true == 0) & (y_pred == 0)).sum()\n",
        "    FP = ((y_true == 0) & (y_pred == 1)).sum()\n",
        "    TP = ((y_true == 1) & (y_pred == 1)).sum()\n",
        "    FN = ((y_true == 1) & (y_pred == 0)).sum()\n",
        "\n",
        "    return (TP * TN - FN * FP) / math.sqrt((TN+FN)*(TN+FP)*(TP+FN)*(TP+FP) + eps)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFBRSrN7kGlR"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "models = [\n",
        "    (\"SVM\", SVC(probability=True)),\n",
        "    (\"LightGBM\", LGBMClassifier())\n",
        "]\n",
        "\n",
        "# Define hyperparameters grid for each model\n",
        "param_grids = {\n",
        "    \"XGBoost\": {\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.1, 0.01, 0.001],\n",
        "        'gamma': [0, 0.1, 0.2],\n",
        "    },\n",
        "    \"RandomForest\": {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "    },\n",
        "    \"LogisticRegression\": {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'solver': ['lbfgs', 'liblinear']\n",
        "    },\n",
        "    \"SVM\": {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    },\n",
        "    \"LightGBM\": {\n",
        "        'num_leaves': [31, 50, 100],\n",
        "        'learning_rate': [0.1, 0.01, 0.001],\n",
        "        'n_estimators': [100, 200, 300]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEKi8IvGcGwT"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve , make_scorer, matthews_corrcoef\n",
        "\n",
        "# Create custom scorers using make_scorer\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'auroc': 'roc_auc',\n",
        "    'sensitivity': make_scorer(sensitivity),\n",
        "    'specificity': make_scorer(specificity),\n",
        "    'mcc': make_scorer(matthews_coeff)\n",
        "}\n",
        "\n",
        "best_params_models = []\n",
        "for model_name, model in models:\n",
        "    # Create GridSearchCV instance\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=5, scoring=scoring, refit='accuracy')\n",
        "\n",
        "    # Perform grid search\n",
        "    grid_search.fit(semantic_vects_train, y_train)\n",
        "\n",
        "    # Print best parameters and best score\n",
        "    print(f\"Best parameters for {model_name}:\", grid_search.best_params_)\n",
        "    print(f\"Best score for {model_name}:\", grid_search.best_score_)\n",
        "\n",
        "    best_params_models.append((model_name, grid_search.best_params_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVP-5pHxcGwU",
        "outputId": "3f8d3de7-7f15-4f8f-931c-1aa54667c418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9097710330138445\n",
            "AUROC: 0.6740686596543544\n",
            "Matthexs coefficient : nan\n",
            "Sensitivity: nan\n",
            "Specificity: nan\n",
            "Confusion Matrix:\n",
            "[[34028   227]\n",
            " [ 3162   143]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_8115/3204507461.py:32: RuntimeWarning: invalid value encountered in divide\n",
            "  return (TP * TN - FN * FP) / math.sqrt((TN+FN)*(TN+FP)*(TP+FN)*(TP+FP))\n",
            "/tmp/ipykernel_8115/3204507461.py:20: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return TP / (TP + FN)\n",
            "/tmp/ipykernel_8115/3204507461.py:25: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return TN / (TN + FP)\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "xgb_model = XGBClassifier(**best_parameters)\n",
        "\n",
        "\n",
        "# Train the model on the entire training dataset\n",
        "xgb_model.fit(semantic_vects_train, y_train)\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_model.predict(semantic_vects_test)\n",
        "y_pred_proba = xgb_model.predict_proba(semantic_vects_test)[:, 1]  # Predict probabilities for positive class\n",
        "\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "auroc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"AUROC:\", auroc)\n",
        "\n",
        "matthews_coeff = matthews_coeff(y_test, y_pred)\n",
        "print(\"Matthews coefficient :\", matthews_coeff)\n",
        "\n",
        "sensitivity = sensitivity(y_test, y_pred)\n",
        "print(\"Sensitivity:\", sensitivity)\n",
        "\n",
        "specificity = specificity(y_test, y_pred)\n",
        "print(\"Specificity:\", specificity)\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb58dGC4cGwV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auroc)\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Plot the diagonal line\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRurbxX_cGwW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "preds = model.predict(dtest_reg)\n",
        "\n",
        "rmse = mean_squared_error(y_test, preds, squared=False)\n",
        "\n",
        "print(f\"RMSE of the base model: {rmse:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvjRHwBxg5La"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "best_params_models.append((\"XGBoost\" ,{'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 5}))\n",
        "best_params_models.append((\"RandomForest\", {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100}))\n",
        "best_params_models.append((\"LogisticRegression\" , {'C': 10, 'solver': 'lbfgs'}))\n",
        "\n",
        "models = [\n",
        "    (\"XGBoost\", XGBClassifier(objective='binary:logistic', n_estimators=100)),\n",
        "    (\"RandomForest\", RandomForestClassifier()),\n",
        "    (\"LogisticRegression\", LogisticRegression(max_iter=1000)),\n",
        "    (\"SVM\", SVC(probability=True)),\n",
        "    (\"LightGBM\", LGBMClassifier())\n",
        "]\n",
        "\n",
        "\n",
        "# Plot ROC curves for the best models\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for model_name, model in models:\n",
        "    best_params = [params for name, params in best_params_models if name == model_name][0]\n",
        "    model.set_params(**best_params)\n",
        "\n",
        "    model.fit(semantic_vects_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(semantic_vects_test)\n",
        "    y_pred_proba = model.predict_proba(semantic_vects_test)[:, 1]  # Predict probabilities for positive class\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "    plt.plot(fpr, tpr, label=f'{model_name}: ROC curve (area = %0.2f)' % auroc)\n",
        "\n",
        "# Add the chance line\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--', label='Chance')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve Comparison')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of pattern for RNA-binding interfaces"
      ],
      "metadata": {
        "id": "bS7yowHGvofk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oviZ4OM-THYR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.spatial.distance import cosine\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "\n",
        "# Function to alter a certain percentage of residues in a sequence\n",
        "def alter_sequence(sequence, alter_percent):\n",
        "    sequence = list(sequence)\n",
        "    num_to_alter = int(len(sequence) * alter_percent / 100)\n",
        "    positions = random.sample(range(len(sequence)), num_to_alter)\n",
        "    for pos in positions:\n",
        "        sequence[pos] = random.choice('ACDEFGHIKLMNPQRSTVWY')  # 20 standard amino acids\n",
        "    return ''.join(sequence)\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return 1 - cosine(vec1, vec2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veovaNmiTRUO"
      },
      "outputs": [],
      "source": [
        "original_sequences = df_train['Sequence'].tolist()\n",
        "\n",
        "# Generate datasets with altered sequences\n",
        "alter_percentages = [40, 45, 50, 55]\n",
        "altered_datasets = {percent: [] for percent in alter_percentages}\n",
        "\n",
        "for seq in original_sequences:\n",
        "    for percent in alter_percentages:\n",
        "        altered_seq = alter_sequence(seq, percent)\n",
        "        altered_datasets[percent].append(altered_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec models on each altered dataset\n",
        "word2vec_models = {}\n",
        "for percent, sequences in altered_datasets.items():\n",
        "    # Tokenize sequences for Word2Vec (simple tokenization by splitting each residue as a token)\n",
        "    tokenized_sequences = [[residue for residue in seq] for seq in sequences]\n",
        "    model = Word2Vec(sentences=tokenized_sequences, vector_size=100, window=5, min_count=1, sg=1)\n",
        "    word2vec_models[percent] = model\n"
      ],
      "metadata": {
        "id": "zGFpYLI1v4Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "semantic_relationships = {percent: [] for percent in alter_percentages}\n",
        "\n",
        "for percent, model_shuff in word2vec_models.items():\n",
        "    for aa1 in amino_acids:\n",
        "        for aa2 in amino_acids:\n",
        "            if aa1 != aa2:\n",
        "                vec1 = model_shuff.wv[aa1]\n",
        "                vec2 = model_shuff.wv[aa2]\n",
        "                similarity = cosine_similarity(vec1, vec2)\n",
        "                semantic_relationships[percent].append(similarity)\n",
        "\n",
        "# Compare semantic relationships using Student's t-test\n",
        "p_value_matrix = pd.DataFrame(index=alter_percentages, columns=alter_percentages)\n",
        "\n",
        "for i, percent1 in enumerate(alter_percentages):\n",
        "    for j, percent2 in enumerate(alter_percentages):\n",
        "        if percent1 != percent2:\n",
        "            t_stat, p_val = ttest_ind(semantic_relationships[percent1], semantic_relationships[percent2])\n",
        "            p_value_matrix.at[percent1, percent2] = p_val\n",
        "        else:\n",
        "            p_value_matrix.at[percent1, percent2] = 1  # Fill diagonal with NaN as self-comparison is not needed\n"
      ],
      "metadata": {
        "id": "xAPWoYmmwT9X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}