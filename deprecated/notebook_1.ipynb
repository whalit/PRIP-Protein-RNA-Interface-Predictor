{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2K66vKOcGwE"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MazsGprpcGwH"
      },
      "outputs": [],
      "source": [
        "from  utils import read_txt_file, write_csv_file\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eldX2yFcGwJ"
      },
      "source": [
        "# Materials and Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USECRE_KcGwJ"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqBDss3AcGwK"
      },
      "source": [
        "The dataset **RB198** was employed as the training set, while **RB111** served as the independent set in this implementation.\n",
        "\n",
        " Both datasets were acquired from the following source: http://ailab-projects2.ist.psu.edu/RNABindRPlus/data.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X13nkSp4cGwK"
      },
      "source": [
        "Originally, the data was structured as a text file in the following format:\n",
        "\n",
        "```\n",
        "#First line: PDBID and Chain ID\n",
        "#Second line: Sequence\n",
        "#Third line: Interface residues defined using a 5.0 angstrom distance cut-off\n",
        "2XFZ_Y\n",
        "KGFKDYGHDYHPAPKTENIKGLGDLKPGIPKTPKQNGGGKRKRWTGDKGRKIYEWDSQAGELEGYRASDGQHLGSFDPKTGNQLKGPDPKRNIKKYL\n",
        "0000000011100000000000000000111011111111110000010110000111100000010110000000000000000000111101111\n",
        "```\n",
        "\n",
        "We needed to convert this data into a CSV file with the following features:\n",
        "\n",
        "**PDBID**, **ChainID**, **Sequence**, **Interface**\n",
        "\n",
        "The script processed this transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN050rE9cGwL"
      },
      "outputs": [],
      "source": [
        "rb198txt = 'Datasets/RB198.txt'\n",
        "rb198 = 'Datasets/RB198.csv'\n",
        "\n",
        "data = read_txt_file(rb198txt)\n",
        "write_csv_file(data, rb198)\n",
        "\n",
        "rb111txt = 'Datasets/RB111.txt'\n",
        "rb111 = 'Datasets/RB111.csv'\n",
        "data = read_txt_file(rb111txt)\n",
        "write_csv_file(data, rb111)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN1KUEjIcGwL"
      },
      "source": [
        "The structure of the dataframe resembles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EH9UQCa8cGwM",
        "outputId": "83a2d001-514e-4ff2-c834-dd7f7b39c231"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PDBID</th>\n",
              "      <th>ChainID</th>\n",
              "      <th>Sequence</th>\n",
              "      <th>Interface</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2AZ0</td>\n",
              "      <td>A</td>\n",
              "      <td>MPSKLALIQELPDRIQTAVEAAMGMSYQDAPNNVRRDLDNLHACLN...</td>\n",
              "      <td>0000000000000000000000000000000010011001001100...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1M8V</td>\n",
              "      <td>A</td>\n",
              "      <td>GAMAERPLDVIHRSLDKDVLVILKKGFEFRGRLIGYDIHLNVVLAD...</td>\n",
              "      <td>0001110110010010000000000000000001111110000000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2PJP</td>\n",
              "      <td>A</td>\n",
              "      <td>FSEEQQAIWQKAEPLFGDEPWWVRDLAKETGTDEQAMRLTLRQAAQ...</td>\n",
              "      <td>0000000000000000000001110000000001000100000000...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  PDBID ChainID                                           Sequence  \\\n",
              "0  2AZ0       A  MPSKLALIQELPDRIQTAVEAAMGMSYQDAPNNVRRDLDNLHACLN...   \n",
              "1  1M8V       A  GAMAERPLDVIHRSLDKDVLVILKKGFEFRGRLIGYDIHLNVVLAD...   \n",
              "2  2PJP       A  FSEEQQAIWQKAEPLFGDEPWWVRDLAKETGTDEQAMRLTLRQAAQ...   \n",
              "\n",
              "                                           Interface  \n",
              "0  0000000000000000000000000000000010011001001100...  \n",
              "1  0001110110010010000000000000000001111110000000...  \n",
              "2  0000000000000000000001110000000001000100000000...  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = pd.read_csv('Datasets/RB198.csv')\n",
        "train_data.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjr-sFSicGwO"
      },
      "source": [
        "## Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGu-F_BtcGwO"
      },
      "source": [
        "In this implementation, the proposed PRIP method comprised five steps:\n",
        "\n",
        "1. Pre-training the Word2vec model.\n",
        "2. Dividing protein sequences.\n",
        "3. Extracting semantic features.\n",
        "4. Training the XGBoost classifier.\n",
        "5. Discerning between binding and non-binding sites.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lolfWfSQcGwP"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "N265eb69cGwP",
        "outputId": "4517eff2-099e-4208-d54c-718c3b9fca00"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_path_here/RB198.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0bf6ea243da9>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the RB198 dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrb198_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'your_path_here/RB198.csv'\u001b[0m  \u001b[0;31m# Replace with your actual path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrb198_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrb198_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Tokenize the sequences into \"words\" (here, each amino acid is considered a word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_path_here/RB198.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import PathLineSentences\n",
        "\n",
        "# Load the RB198 dataset\n",
        "rb198_path = 'your_path_here/RB198.csv'  # Replace with your actual path\n",
        "rb198_data = pd.read_csv(rb198_path)\n",
        "\n",
        "# Tokenize the sequences into \"words\" (here, each amino acid is considered a word)\n",
        "tokenized_sequences = [list(sequence) for sequence in train_data['Sequence']]\n",
        "\n",
        "# Define the Word2vec model with the specified parameters\n",
        "model = Word2Vec(sentences=tokenized_sequences,\n",
        "                vector_size=25,           # Dimensionality of the word vectors\n",
        "                window=5,                 # Maximum distance between the current and predicted word within a sentence\n",
        "                min_count=1,              # Ignores all words with total frequency lower than this\n",
        "                sg=0,                     # Use CBOW model\n",
        "                negative=5,               # Number of negative samples\n",
        "                epochs=200,               # Number of iterations (epochs) over the corpus\n",
        "                workers=1)                # Number of worker threads\n",
        "\n",
        "# Save the model for later use or to load it in another environment\n",
        "model.save('/mnt/data/word2vec_protein_sequences.model')\n",
        "\n",
        "# Example usage: Getting the vector for a specific amino acid\n",
        "vector_for_amino_acid_A = model.wv['A']\n",
        "print(vector_for_amino_acid_A)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "km48R1BVcGwQ"
      },
      "outputs": [],
      "source": [
        "def split_protein_sequence(protein_sequence, segment_length):\n",
        "\n",
        "    if segment_length % 2 == 0 :\n",
        "        raise ValueError(\"Segment length must be an odd number and greater than or equal to 2*n + 1\")\n",
        "\n",
        "    segments = []\n",
        "    sequence_length = len(protein_sequence)\n",
        "\n",
        "    n = (segment_length - 1)// 2\n",
        "\n",
        "    # Iterate over each residue in the protein sequence\n",
        "    for i in range(sequence_length):\n",
        "        # Determine the start and end indices of the segment\n",
        "        start_index = max(0, i - n)\n",
        "        end_index = min(sequence_length, i + n + 1)\n",
        "\n",
        "        # Pad the segment with \"X\" characters at the start or end if needed\n",
        "        if start_index == 0:\n",
        "            padded_segment = \"X\" * (n - i) + protein_sequence[:end_index]\n",
        "        elif end_index == sequence_length:\n",
        "            padded_segment = protein_sequence[start_index:] + \"X\" * (n - (sequence_length - 1 - i))\n",
        "        else:\n",
        "            segment = protein_sequence[start_index:end_index]\n",
        "            padded_segment = \"X\" * (n - (i - start_index)) + segment\n",
        "\n",
        "        # Add the segment to the list of segments\n",
        "        segments.append(padded_segment)\n",
        "\n",
        "    return segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jt5xtsdjcGwQ"
      },
      "outputs": [],
      "source": [
        "# Concatenate semantic vectors for each residue\n",
        "def feature_extract(segments , W2V_model):\n",
        "\n",
        "    semantic_vect_concat = []\n",
        "    for segment in segments :\n",
        "        seg_vect = []\n",
        "        for residue in segment :\n",
        "            if residue != 'X' :\n",
        "                seg_vect.extend(W2V_model.wv[residue])\n",
        "            else:\n",
        "                seg_vect.extend([0 for i in range(25)])\n",
        "        semantic_vect_concat.append(seg_vect)\n",
        "\n",
        "    return semantic_vect_concat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fdn6MQXAcGwR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /home/zak/.local/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/lib/python3/dist-packages (from gensim) (1.8.0)\n",
            "Collecting smart-open>=1.8.1\n",
            "  Using cached smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
            "Collecting wrapt\n",
            "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, smart-open, gensim\n",
            "Successfully installed gensim-4.3.2 smart-open-7.0.4 wrapt-1.16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "! pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "\n",
        "def load_model(path):\n",
        "    return Word2Vec.load(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MlfVq-lpcGwR"
      },
      "outputs": [],
      "source": [
        "model = load_model(\"model_W2V.model\")\n",
        "\n",
        "df_train = pd.read_csv(\"Datasets/RB198.csv\")\n",
        "df_test = pd.read_csv(\"Datasets/RB111.csv\")\n",
        "\n",
        "protein_seqs_train = df_train['Sequence'].to_list()\n",
        "label_interfaces_train = df_train['Interface'].to_list()\n",
        "\n",
        "\n",
        "protein_seqs_test= df_test['Sequence'].to_list()\n",
        "label_interfaces_test = df_test['Interface'].to_list()\n",
        "\n",
        "pairs_train = list(zip(protein_seqs_train , label_interfaces_train))\n",
        "pairs_test = list(zip(protein_seqs_test , label_interfaces_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GzptIZUFcGwS"
      },
      "outputs": [],
      "source": [
        "# Sequence splitting + assigning the interface labels for each split\n",
        "X_train , y_train  , X_test , y_test = [] , [] , [] , []\n",
        "segment_length = 11\n",
        "n = 3\n",
        "for elem in pairs_train :\n",
        "    prot_sequence = elem[0]\n",
        "    label_interf = elem[1]\n",
        "\n",
        "    if len(prot_sequence) < segment_length :\n",
        "        print(\"inf\")\n",
        "    segments = split_protein_sequence(prot_sequence ,segment_length)\n",
        "\n",
        "    # X represents the segments (sequences split) and y represents the respective labels\n",
        "    X_train.extend(segments)\n",
        "    y_train.extend(label_interf)\n",
        "\n",
        "for elem in pairs_test :\n",
        "    prot_sequence = elem[0]\n",
        "    label_interf = elem[1]\n",
        "\n",
        "    if len(prot_sequence) < segment_length :\n",
        "        print(\"inf\")\n",
        "    segments = split_protein_sequence(prot_sequence ,segment_length)\n",
        "\n",
        "    # X represents the segments (sequences split) and y represents the respective labels\n",
        "    X_test.extend(segments)\n",
        "    y_test.extend(label_interf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Sir4RDQWcGwS"
      },
      "outputs": [],
      "source": [
        "semantic_vects_train = feature_extract(X_train , model)\n",
        "semantic_vects_test = feature_extract(X_test , model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QyvLvL7gcGwT"
      },
      "outputs": [],
      "source": [
        "y_train_int = [int(x) for x in y_train]\n",
        "y_test_int = [int(x) for x in y_test]\n",
        "\n",
        "y_train = y_train_int\n",
        "y_test = y_test_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "skNm8NMQcGwT"
      },
      "outputs": [],
      "source": [
        "def sensitivity(y_true, y_pred):\n",
        "    eps = 1e-8\n",
        "    TP = ((y_true == 1) & (y_pred == 1)).sum()\n",
        "    FN = ((y_true == 1) & (y_pred == 0)).sum()\n",
        "    return TP / (TP + FN + eps)\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    eps = 1e-8\n",
        "    TN = ((y_true == 0) & (y_pred == 0)).sum()\n",
        "    FP = ((y_true == 0) & (y_pred == 1)).sum()\n",
        "    return TN / (TN + FP + eps)\n",
        "def matthews_coeff(y_true , y_pred):\n",
        "    eps = 1e-8\n",
        "    TN = ((y_true == 0) & (y_pred == 0)).sum()\n",
        "    FP = ((y_true == 0) & (y_pred == 1)).sum()\n",
        "    TP = ((y_true == 1) & (y_pred == 1)).sum()\n",
        "    FN = ((y_true == 1) & (y_pred == 0)).sum()\n",
        "\n",
        "    return (TP * TN - FN * FP) / math.sqrt((TN+FN)*(TN+FP)*(TP+FN)*(TP+FP) + eps)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wFBRSrN7kGlR"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "models = [\n",
        "    (\"SVM\", SVC(probability=True)),\n",
        "    (\"LightGBM\", LGBMClassifier())\n",
        "]\n",
        "\n",
        "# Define hyperparameters grid for each model\n",
        "param_grids = {\n",
        "    \"XGBoost\": {\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.1, 0.01, 0.001],\n",
        "        'gamma': [0, 0.1, 0.2],\n",
        "    },\n",
        "    \"RandomForest\": {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "    },\n",
        "    \"LogisticRegression\": {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'solver': ['lbfgs', 'liblinear']\n",
        "    },\n",
        "    \"SVM\": {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    },\n",
        "    \"LightGBM\": {\n",
        "        'num_leaves': [31, 50, 100],\n",
        "        'learning_rate': [0.1, 0.01, 0.001],\n",
        "        'n_estimators': [100, 200, 300]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEKi8IvGcGwT",
        "outputId": "17807a79-f62e-4939-8e80-1906447dcd35"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve , make_scorer, matthews_corrcoef\n",
        "\n",
        "# Create custom scorers using make_scorer\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'auroc': 'roc_auc',\n",
        "    'sensitivity': make_scorer(sensitivity),\n",
        "    'specificity': make_scorer(specificity),\n",
        "    'mcc': make_scorer(matthews_coeff)\n",
        "}\n",
        "\n",
        "best_params_models = []\n",
        "for model_name, model in models:\n",
        "    # Create GridSearchCV instance\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=5, scoring=scoring, refit='accuracy')\n",
        "\n",
        "    # Perform grid search\n",
        "    grid_search.fit(semantic_vects_train, y_train)\n",
        "\n",
        "    # Print best parameters and best score\n",
        "    print(f\"Best parameters for {model_name}:\", grid_search.best_params_)\n",
        "    print(f\"Best score for {model_name}:\", grid_search.best_score_)\n",
        "\n",
        "    best_params_models.append((model_name, grid_search.best_params_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVP-5pHxcGwU",
        "outputId": "3f8d3de7-7f15-4f8f-931c-1aa54667c418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9097710330138445\n",
            "AUROC: 0.6740686596543544\n",
            "Matthexs coefficient : nan\n",
            "Sensitivity: nan\n",
            "Specificity: nan\n",
            "Confusion Matrix:\n",
            "[[34028   227]\n",
            " [ 3162   143]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_8115/3204507461.py:32: RuntimeWarning: invalid value encountered in divide\n",
            "  return (TP * TN - FN * FP) / math.sqrt((TN+FN)*(TN+FP)*(TP+FN)*(TP+FP))\n",
            "/tmp/ipykernel_8115/3204507461.py:20: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return TP / (TP + FN)\n",
            "/tmp/ipykernel_8115/3204507461.py:25: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return TN / (TN + FP)\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "xgb_model = XGBClassifier(**best_parameters)\n",
        "\n",
        "\n",
        "# Train the model on the entire training dataset\n",
        "xgb_model.fit(semantic_vects_train, y_train)\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_model.predict(semantic_vects_test)\n",
        "y_pred_proba = xgb_model.predict_proba(semantic_vects_test)[:, 1]  # Predict probabilities for positive class\n",
        "\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "auroc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"AUROC:\", auroc)\n",
        "\n",
        "matthews_coeff = matthews_coeff(y_test, y_pred)\n",
        "print(\"Matthews coefficient :\", matthews_coeff)\n",
        "\n",
        "sensitivity = sensitivity(y_test, y_pred)\n",
        "print(\"Sensitivity:\", sensitivity)\n",
        "\n",
        "specificity = specificity(y_test, y_pred)\n",
        "print(\"Specificity:\", specificity)\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb58dGC4cGwV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auroc)\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Plot the diagonal line\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRurbxX_cGwW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "preds = model.predict(dtest_reg)\n",
        "\n",
        "rmse = mean_squared_error(y_test, preds, squared=False)\n",
        "\n",
        "print(f\"RMSE of the base model: {rmse:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvjRHwBxg5La"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "best_params_models.append((\"XGBoost\" ,{'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 5}))\n",
        "best_params_models.append((\"RandomForest\", {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100}))\n",
        "best_params_models.append((\"LogisticRegression\" , {'C': 10, 'solver': 'lbfgs'}))\n",
        "\n",
        "models = [\n",
        "    (\"XGBoost\", XGBClassifier(objective='binary:logistic', n_estimators=100)),\n",
        "    (\"RandomForest\", RandomForestClassifier()),\n",
        "    (\"LogisticRegression\", LogisticRegression(max_iter=1000)),\n",
        "    (\"SVM\", SVC(probability=True)),\n",
        "    (\"LightGBM\", LGBMClassifier())\n",
        "]\n",
        "\n",
        "\n",
        "# Plot ROC curves for the best models\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for model_name, model in models:\n",
        "    best_params = [params for name, params in best_params_models if name == model_name][0]\n",
        "    model.set_params(**best_params)\n",
        "\n",
        "    model.fit(semantic_vects_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(semantic_vects_test)\n",
        "    y_pred_proba = model.predict_proba(semantic_vects_test)[:, 1]  # Predict probabilities for positive class\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "    plt.plot(fpr, tpr, label=f'{model_name}: ROC curve (area = %0.2f)' % auroc)\n",
        "\n",
        "# Add the chance line\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--', label='Chance')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve Comparison')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oviZ4OM-THYR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Function to alter a certain percentage of residues in a sequence\n",
        "def alter_sequence(sequence, alter_percent):\n",
        "    sequence = list(sequence)\n",
        "    num_to_alter = int(len(sequence) * alter_percent / 100)\n",
        "    positions = random.sample(range(len(sequence)), num_to_alter)\n",
        "    for pos in positions:\n",
        "        sequence[pos] = random.choice('ACDEFGHIKLMNPQRSTVWY')  # 20 standard amino acids\n",
        "    return ''.join(sequence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veovaNmiTRUO"
      },
      "outputs": [],
      "source": [
        "original_sequences = df_train['Sequence'].tolist()\n",
        "\n",
        "# Generate datasets with altered sequences\n",
        "alter_percentages = [40, 45, 50, 55]\n",
        "altered_datasets = {percent: [] for percent in alter_percentages}\n",
        "\n",
        "for seq in original_sequences:\n",
        "    for percent in alter_percentages:\n",
        "        altered_seq = alter_sequence(seq, percent)\n",
        "        altered_datasets[percent].append(altered_seq)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
